{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE311K Final Project Submission\n",
    "\n",
    "## *Abstract*\n",
    "In this project, we seek to create a VRP solver. The VRP solver will be generic in nature, and will take in a generalized spreadsheet. As a result, the only constraint on what can and cannot be solved by our code will be whether or not the address is registered with Google and TomTom, the two API's utilized in this project. \n",
    "\n",
    "## *Project Steps*\n",
    "* Prep Data by adding coordinates\n",
    "* Describe constraints as numpy arrays\n",
    "* Write contraints to LP file format\n",
    "* Solve LP File\n",
    "* Interpret solution\n",
    "* Graph solution\n",
    "\n",
    "### *Libraries Used*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import plotly as plt\n",
    "import plotly.graph_objects as go\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Prepping Data*\n",
    "The data intake will be in a fairly generic form. There are 2 columns: `Name` and `Address`. For later graphing, we will also need the coordinates of all our locations. Therefore, let us write a quick function to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'Whatever File You Wish To Use' # This is the name of the .csv file you wish to use. DO NOT INCLUDE .csv when specifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the event we receive multiple results from the API, let us create a handler. Since this is used later by the API caller, we need to define the function now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note that this was all written in several scripts, and then dumped into a jupyer notebook.\n",
    "# For best results, please ask Stephen Smith (UT EID: sjs5555, email: stephen.smith@utexas.edu) for a copy of his API keys and git repo\n",
    "def MultipleResultsHandler(response:dict) -> int:\n",
    "    ''' Has the user select which response they want from the API call when there are several\n",
    "\n",
    "    # Arguments #\n",
    "    :arg response: All of the possible responses from the API\n",
    "    :type response: dict\n",
    "\n",
    "    # Returns #\n",
    "    :ret selection: The choice of the user\n",
    "    :rtype selection: int\n",
    "    '''\n",
    "    location = response[\"summary\"][\"query\"]\n",
    "    print(f\"There were multiple results querying for {location}, please select an option.\\n------------------------------------------------------------------------------------\")\n",
    "    for index,item in enumerate(response[\"results\"]):\n",
    "        item_address = item[\"address\"][\"freeformAddress\"]\n",
    "        print(f\"{index}) {item_address}\")\n",
    "        \n",
    "    print(\"------------------------------------------------------------------------------------\")\n",
    "    selection = -1\n",
    "    while (selection not in range(response[\"summary\"][\"numResults\"])):\n",
    "        selection = input(\"Please select an option from the ones given above: \")\n",
    "        try:\n",
    "            selection = int(selection)\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid digit from the given options.\")\n",
    "    return int(selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the exception has been handled, let us write the API call function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PopulateDataframe(df:pd.DataFrame,apiKey:str,pickFirst:bool=False) -> pd.DataFrame:\n",
    "    ''' Takes a dataframe of addresses, and gives it coordinates\n",
    "\n",
    "    # Arguments #\n",
    "    :arg df: Loaded .csv file with all of the locations wanted\n",
    "    :type df: pd.DataFrame\n",
    "    :arg apiKey: TomTom api key\n",
    "    :type apiKey: str\n",
    "    :arg pickFirst: Option for user to just always pick the first option, instead of manually picking each option. Defaults to False\n",
    "    :type pickFirst: bool\n",
    "\n",
    "    # Returns #\n",
    "    :ret df: Updated dataframe with all of the associated coordinates\n",
    "    :rtype df pd.DataFrame\n",
    "    '''\n",
    "    assert 'Address' in df.columns, 'Ensure that the Dataframe provided contains a column called \"Name\".'\n",
    "    df[\"Latitude\"] = np.nan\n",
    "    df[\"Longitude\"] = np.nan\n",
    "    for i in range(len(df)):\n",
    "        location = df.iloc[i][\"Address\"]\n",
    "        if pickFirst:\n",
    "            response = requests.get(f\"https://api.tomtom.com/search/2/geocode/{location}.json?limit=1&key={apiKey}\").json()\n",
    "        else:\n",
    "            response = requests.get(f\"https://api.tomtom.com/search/2/geocode/{location}.json?key={apiKey}\").json()\n",
    "        assert response[\"summary\"][\"numResults\"] > 0, f\"There were no results returned after querying for {location}, are you sure that's a valid location?\"\n",
    "        if (response[\"summary\"][\"numResults\"] > 1):\n",
    "            selection = MultipleResultsHandler(response)\n",
    "            df.at[i,\"Latitude\"] = response[\"results\"][selection][\"position\"][\"lat\"]\n",
    "            df.at[i,\"Longitude\"] = response[\"results\"][selection][\"position\"][\"lon\"]\n",
    "        else:\n",
    "            df.at[i,\"Latitude\"] = response[\"results\"][0][\"position\"][\"lat\"]\n",
    "            df.at[i,\"Longitude\"] = response[\"results\"][0][\"position\"][\"lon\"]\n",
    "    # Overwrite old file and return DataFrame\n",
    "    path = os.path.join(os.getcwd(),'Data',FILENAME+'.csv')\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    df.to_csv(path, index='False')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the function to prep the data, let us go ahead and do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load needed data, apiKey\n",
    "df = pd.read_csv(os.path.join(os.getcwd(),'Data',FILENAME+'.csv'))\n",
    "with open(os.path.join(os.getcwd(),'Keys.json'), 'r') as f:\n",
    "    keys = json.load(f)\n",
    "# Update the dataframe\n",
    "coords = PopulateDataframe(df, keys['TomTom'])\n",
    "print(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Modeling the Data*\n",
    "VRP modeling is relatively straight forward. For reference, conduct a literature review. A helpful video might be [this one](https://www.youtube.com/watch?v=-hGL39jdtQE&ab_channel=Hern%C3%A1nC%C3%A1ceres). We will use a cost matrix and 3 constraints:\n",
    "* All locations must be visited exactly once\n",
    "* All locations must have exactly one entering and one exiting vehicle\n",
    "* Subtours must be strictly prohibited\n",
    "\n",
    "For subtour elimination, we will use the Miller, Tucker, Zemlin Approach, [as described here.](https://faculty.math.illinois.edu/~mlavrov/slides/482-spring-2020/slides35.pdf)\n",
    "\n",
    "### First, let us create a distance matrix\n",
    "This will be done by creating iterating through the dataset previously made, and entering the values into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API Call Function\n",
    "def RouteCaller(loc1:str, loc2:str,  key:str=None, units:str='meters', routeType:str='BICYCLE') -> float:\n",
    "    \"\"\" Function that makes a single API call between two distances\n",
    "\n",
    "    # Arguments #\n",
    "    :arg loc1: Starting location for the route\n",
    "    :type loc1: str\n",
    "    :arg loc2: Ending location for the route\n",
    "    :type loc2: str\n",
    "    :arg units: Measuring system for the route, metric of imperial\n",
    "    :type units: str\n",
    "    :arg routeType: Mode of travel for the route. Default is Bicyle because that's the purpose of this project\n",
    "    :type routeType: str\n",
    "    \"\"\"\n",
    "    print(f'Getting data for {loc1} to {loc2}')\n",
    "    # Verify Key exists\n",
    "    if key == None:\n",
    "        raise ValueError('No API Key has been passed. Please insert key and try again')\n",
    "    # Ping the API\n",
    "    retVal = requests.get(\n",
    "        f\"https://maps.googleapis.com/maps/api/directions/json?destination={loc1}&origin={loc2}&units={units}&mode={routeType}&key={key}\").json()\n",
    "    if 'error_message' in retVal.keys():\n",
    "        # Error handling\n",
    "        print(\n",
    "            f\"Error when pinging API, issue: {retVal['error_message']}\")\n",
    "        return np.NaN\n",
    "    else:\n",
    "        # Return the route information\n",
    "        return float(retVal['routes'][0]['legs'][0]['distance']['text'][:-3])\n",
    "\n",
    "# Define the Distance Matrix\n",
    "def generateDistanceMatrix(df:pd.DataFrame, key:str) -> np.ndarray:\n",
    "    \"\"\" Takes in a DataFrame, and returns a numpy array\n",
    "\n",
    "    # Arguments #\n",
    "    :arg df: all of the addresses in the problem, placed in a pandas DataFrame\n",
    "    :type df: pd.DataFrame\n",
    "    :arg key: API key, so that Google knows who is calling\n",
    "    :type key: str\n",
    "\n",
    "    # Returns #\n",
    "    :return distMatrix: A matrix of size NxN, where N is the number of locations in DataFrame df\n",
    "    :rtype distMatrix: np.ndarray\n",
    "    \"\"\"\n",
    "    # Local array storage\n",
    "    distDict = {}\n",
    "    arrays = []\n",
    "    i = 0\n",
    "    # Iterate through the DataFrame\n",
    "    for index1, row1 in df.iterrows():\n",
    "        distDict[index1] = []\n",
    "        for index2, row2 in df.iterrows():\n",
    "            if index1 == index2:\n",
    "                # If location equals itself then give 0 for distance, no need to ping the API\n",
    "                distDict[index1].append(0)\n",
    "\n",
    "            elif index1 != index2:\n",
    "                i += 1\n",
    "                # Ping Google's Destination API for route data\n",
    "                distDict[index1].append(RouteCaller(df.iloc[index1]['Address'], df.iloc[index2]['Address'], key))\n",
    "    print(f'Successfully made {i} calls to the Google Maps Route API')\n",
    "    # Process data into arrays, stack, and return\n",
    "    for key in distDict.keys():\n",
    "        arrays.append(np.array(distDict[key]))\n",
    "    distMatrix = np.vstack(arrays)\n",
    "    return distMatrix\n",
    "\n",
    "\n",
    "def validateCache(filename:str,df:pd.DataFrame,key:str=None) -> np.ndarray:\n",
    "    \"\"\" Validate and load cached data, to prevent unnecessary API calls\n",
    "\n",
    "    # Arguments #\n",
    "    :arg filename: ending of filename to check if it exits. If it doesn't we will end up making it\n",
    "    :type filename: str\n",
    "    :arg df: If no cache, need data to pass to route generator\n",
    "    :type df: pd.DataFrame\n",
    "    :arg key: If no cache, need key for API call\n",
    "    :type key: str\n",
    "\n",
    "    # Returns #\n",
    "    :return distMatrix: Returns a distmatrix\n",
    "    :rtype distMatrix: np.ndarray\n",
    "    \"\"\"\n",
    "    path = os.getcwd()\n",
    "    # Define where the cache should be\n",
    "    cache_filename = os.path.splitext(filename)[0] + \".npy\"\n",
    "    # Check for if the parent folder exists, if not make one\n",
    "    if not os.path.exists(os.path.join(os.getcwd(),\"CachedDistances\")):\n",
    "        os.mkdir(os.path.join(path,\"CachedDistances\"))\n",
    "    # Check if the distance matrix doesn't exist, if so make one\n",
    "    if not os.path.exists(os.path.join(path,\"CachedDistances\",cache_filename)):\n",
    "        distMatrix = generateDistanceMatrix(df, key)\n",
    "        np.save(os.path.join(path,\"CachedDistances\",cache_filename),distMatrix)\n",
    "        return distMatrix\n",
    "    # Since the matrix exists, just load it\n",
    "    else:\n",
    "        distMatrix = np.load(os.path.join(path, \"CachedDistances\", cache_filename))\n",
    "        return distMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside: If it is know that routes are symmetric, an optimization could be made by only querying the API if i < j, else set the value to the previously recorded value. However, since one way roads do exist and could be used, it is necessary to assume the VRP is asymetric. \n",
    "\n",
    "### Second, let us define constraints off of the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateContraintMatrix(distMatrix:np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Generate the constraint matrix from given distance matrix\n",
    "\n",
    "    # Arguments #\n",
    "    :arg distMatrix: Big distance matrix that we calculated in the last step\n",
    "    :type distMatrix: np.ndarray\n",
    "\n",
    "    # Returns #\n",
    "    :return retVal: Combined constraint matrix, with top and bottom halves stacked nicely\n",
    "    :rtype retVal: np.ndarray\n",
    "    \"\"\"\n",
    "    NumElements = len(distMatrix)\n",
    "    \"\"\"\n",
    "        Generates the top portion of the constraint matrix, a 1 x N matrix of ones for index i in N. Here is an example with N = 3.\n",
    "        [ 0 0 0 0 0 0 0 0 0 ]        [ 1 1 1 0 0 0 0 0 0 ]\n",
    "        | 0 0 0 0 0 0 0 0 0 | --->   | 0 0 0 1 1 1 0 0 0 |\n",
    "        [ 0 0 0 0 0 0 0 0 0 ]        [ 0 0 0 0 0 0 1 1 1 ]\n",
    "    \"\"\"\n",
    "    retArrayTop = np.zeros(shape=(NumElements,NumElements**2))\n",
    "    for index in range(NumElements):\n",
    "        retArrayTop[index,index*NumElements:(index+1)*NumElements] = np.ones(shape=(1,NumElements))\n",
    "    \"\"\"\n",
    "        Generates the bottom portion of the constraint matrix, a N x N**2 matrix full of horizontally aligned identity N x N matricies.\n",
    "        Example with N = 3.\n",
    "        [ 1 0 0 ]   [ 1 0 0 ]   [ 1 0 0 ]         [ 1 0 0 1 0 0 1 0 0 ]\n",
    "        | 0 1 0 | + | 0 1 0 | + | 0 1 0 |   --->  | 0 1 0 0 1 0 0 1 0 |\n",
    "        [ 0 0 1 ]   [ 0 0 1 ]   [ 0 0 1 ]         [ 0 0 1 0 0 1 0 0 1 ]\n",
    "    \"\"\"\n",
    "    individualBottoms = tuple(np.identity(NumElements) for i in range(NumElements))\n",
    "    retArrayBottom = np.hstack(individualBottoms)\n",
    "    retVal = np.vstack((retArrayTop,retArrayBottom))\n",
    "    #retVal = np.vstack((retVal,np.ones(shape=(1,NumElements**2),dtype=np.uint8)))\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that this does not generate sub-tour elimination constraints. They are defined during the LP generation, as they utilize a different variable set.\n",
    "\n",
    "### Writing the LP File\n",
    "LP files are wonderful for Linear Programming, and will allow us to seamlessly use [NEOS](https://neos-server.org/neos/) as our free solver. Once the LP file is generated, it is a simple process to upload to a headless CPLEX solver and get an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpGenerator(distMatrix:np.ndarray, constraintMatrix:np.ndarray,filename:str) -> None:\n",
    "    \"\"\" Writes the lp file for the constraint matrix. LP Files are the way we give the solver our problem.\n",
    "\n",
    "    # Arguments #\n",
    "    :arg distMatrix: a distance matrix of size N x N. This contains the necessary objective information\n",
    "    :type distMatrix: np.ndarray\n",
    "    :arg constraintMatrix: the constraint matrix with binary variables\n",
    "    :type constraintMatrix: np.ndarray\n",
    "    :arg filename: input filename that we will make the associated lp file for\n",
    "    :type filename: str\n",
    "    \"\"\"\n",
    "    path = os.getcwd()\n",
    "    # Define where the LP file should be\n",
    "    lp_filename = os.path.splitext(filename)[0] + \".lp\"\n",
    "    # Check for if the parent folder exists, if not make one\n",
    "    if not os.path.exists(os.path.join(path,\"LPFiles\")):\n",
    "        os.mkdir(os.path.join(path,\"LPFiles\"))\n",
    "    full_lp_filename = os.path.join(path,\"LPFiles\",lp_filename)\n",
    "    # Delete old lp file of same name\n",
    "    if os.path.exists(full_lp_filename):\n",
    "        os.remove(full_lp_filename)\n",
    "\n",
    "    # Find N\n",
    "    NumElements = len(distMatrix)\n",
    "\n",
    "    # Variable Creation\n",
    "    subtour_vars = {}\n",
    "    vars_dict = {}\n",
    "    for i in range(NumElements):\n",
    "        subtour_vars[i] = f't{i}'\n",
    "        vars_dict[i] = {}\n",
    "        for j in range(NumElements):\n",
    "            vars_dict[i][j] = f'i{i}j{j}'\n",
    "    subtour_total = 0\n",
    "    for i in range(NumElements):\n",
    "        subtour_total += i\n",
    "    # -------------- Objective Writing -------------- #\n",
    "    costs = []\n",
    "    for i in vars_dict.keys():\n",
    "        for j in vars_dict[i].keys():\n",
    "            cost = distMatrix[i, j]\n",
    "            if cost != 0.0:\n",
    "                costs.append(f'{cost} {vars_dict[i][j]}')\n",
    "                costs.append('+')\n",
    "    costs.pop()\n",
    "    # -------------- Constraint Writing -------------- #\n",
    "    lp_constraints = []\n",
    "    # Top Third (One route in)\n",
    "    loc = 0\n",
    "    for i in vars_dict.keys():\n",
    "        local = []\n",
    "        for j in vars_dict[i].keys():\n",
    "            if i != j:\n",
    "                if constraintMatrix[i,j+loc] == 1.0:\n",
    "                    local.append(vars_dict[i][j])\n",
    "                    local.append(' + ')\n",
    "        local.pop()\n",
    "        lp_constraints.append(local)\n",
    "        loc += NumElements\n",
    "    # Second Third (One route out)\n",
    "    loc = 0\n",
    "    for i in vars_dict.keys():\n",
    "        local = []\n",
    "        for j in vars_dict[i].keys():\n",
    "            if i != j:\n",
    "                if constraintMatrix[i+NumElements,j*NumElements+loc] == 1.0:\n",
    "                    local.append(vars_dict[j][i])\n",
    "                    local.append(' + ')\n",
    "        local.pop()\n",
    "        lp_constraints.append(local)\n",
    "        loc += 1\n",
    "    # Bottom Third (Subtour elimination)\n",
    "    subtours =[]\n",
    "    for i in vars_dict.keys():\n",
    "        for j in vars_dict[i].keys():\n",
    "            if i != j:\n",
    "                if i == 0:\n",
    "                    # Set i == 0 as the first location visited\n",
    "                    subtours.append(f'{vars_dict[i][j]} = 1 -> {subtour_vars[i]} = 1')\n",
    "                else:\n",
    "                    # Rest of the subtour elimination clause\n",
    "                    subtours.append(f'{vars_dict[i][j]} = 1 -> {subtour_vars[i]} - {subtour_vars[j]} >= 1')\n",
    "    subtour_final = []\n",
    "    for i in subtour_vars.keys():\n",
    "        subtour_final.append(i)\n",
    "        subtour_final.append(' + ')\n",
    "    subtour_final.pop()\n",
    "    subtour_final.append(f'= {subtour_total}')\n",
    "\n",
    "    # Write the LP file\n",
    "    with open(full_lp_filename, 'x') as lp:\n",
    "        # Objective Section\n",
    "        lp.write('Min \\n')\n",
    "        for eq in costs:\n",
    "            lp.write(f'{eq} ')\n",
    "        # Constraint Section\n",
    "        lp.write('\\nsubject to \\n')\n",
    "        for eq in lp_constraints:\n",
    "            for i in eq:\n",
    "                lp.write(i)\n",
    "            lp.write(' = 1 \\n')        \n",
    "        count = 0\n",
    "        for i in subtours:\n",
    "            lp.write(f'\\nGC{count}: {i}')\n",
    "            count += 1\n",
    "        for i in subtour_final:\n",
    "            lp.write(f'{i}')\n",
    "        # Define Subtour Variable Bounds\n",
    "        lp.write('\\nbounds \\n')\n",
    "        for i in subtour_vars.keys():\n",
    "            lp.write(f'0 <= {subtour_vars[i]} <= {NumElements} \\n')\n",
    "        # Define Variable Types\n",
    "        lp.write('bin \\n')\n",
    "        for i in vars_dict.keys():\n",
    "            for j in vars_dict[i].keys():\n",
    "                if i != j:\n",
    "                    lp.write(f'{vars_dict[i][j]} ')\n",
    "        lp.write('\\nint \\n')\n",
    "        for i in subtour_vars.keys():\n",
    "            lp.write(f'{subtour_vars[i]} ')\n",
    "        lp.write('\\nEND')\n",
    "    # This is messy but it works. I am also unaware of a better method of writing LP files in Python.\n",
    "    # Pulp might be an alternative, but I am unfamiliar with how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Interpreting the Solution*\n",
    "Now that the LP file has been solved, NEOS has produced a .sol file in xml format that needs interpreting. For easy loading, BeautifulSoup4 is the xml reader of choice. Let us load and interpret the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables used later\n",
    "MAP_PATH = MAP_PATH = os.path.join(os.getcwd(), \"Maps\")\n",
    "with open(os.path.join(os.getcwd(), 'Keys.json')) as f:\n",
    "    keys = json.load(f)\n",
    "TomTomKey = keys['TomTom']\n",
    "MapBoxKey = keys['MapBox']\n",
    "\n",
    "# Get the solution into a workable format\n",
    "def solParser(distMatrix: np.ndarray, filename: str) -> dict:\n",
    "    \"\"\" Read the .sol file and return a much easier to work with dictionary over this xml jargon\n",
    "\n",
    "    # Arguments #\n",
    "    :arg distMatrix: \n",
    "    :arg filename: Generic name of the input data file\n",
    "    :type filename: str\n",
    "\n",
    "    # Returns #\n",
    "    :ret solution: Usable solution in dictionary form\n",
    "    :rtype solution: dict\n",
    "    \"\"\"\n",
    "    # Load solution file\n",
    "    with open(os.path.join(os.getcwd(), 'sol', filename+'.sol'), 'r') as f:\n",
    "        data = f.read()\n",
    "    BS_Data = bs(data, 'xml')\n",
    "    # Get N from distMatrix\n",
    "    NumElements = len(distMatrix)\n",
    "    solution = {}\n",
    "    for i in range(NumElements):\n",
    "        for j in range(NumElements):\n",
    "            val = BS_Data.find('variable', {'name': f'i{i}j{j}'})\n",
    "            if val != None:\n",
    "                solution[val.get('name')] = val.get('value')\n",
    "    return solution\n",
    "\n",
    "# Interpret the solution into something readable, print to terminal\n",
    "def solInterpreter(sol: dict, distMatrix: np.ndarray, filename: str) -> dict:\n",
    "    \"\"\" Takes a solution and outputs the route to the terminal\n",
    "\n",
    "    # Arguments #\n",
    "    :arg sol: All of the route variables and associated values for the solution\n",
    "    :type sol: dict\n",
    "    :arg distMatrix: Used to get how many locations there are\n",
    "    :type distMatrix: np.ndarray\n",
    "    :arg filename: Generic name of the input data file\n",
    "    :type filename: str\n",
    "\n",
    "    # Returns #\n",
    "    :ret routes: The start- and end-points of each route\n",
    "    :rtype routes: dict\n",
    "    \"\"\"\n",
    "    # Get names of locations\n",
    "    with open(os.path.join(os.getcwd(), 'Data', filename+'.csv'), \"r\") as data:\n",
    "        df = pd.read_csv(data)\n",
    "    NumElements = len(distMatrix)\n",
    "    routes = {}\n",
    "    print('# ---------- Interpreting Solution ---------- #')\n",
    "    # Get the solution in order, ease of read\n",
    "    for i in range(NumElements):\n",
    "        for j in range(NumElements):\n",
    "            loc = f'i{i}j{j}'\n",
    "            if loc in sol.keys() and int(sol[loc]) == 1:\n",
    "                routes[i] = j\n",
    "    stop = 0\n",
    "    i = 0\n",
    "\n",
    "    # Print solution to terminal\n",
    "    for loc in routes.keys():\n",
    "        if stop != loc:\n",
    "            print(\n",
    "                f'Route contains {df.iloc[routes[stop], 1]} -> {df.iloc[routes[loc], 1]}')\n",
    "            stop = loc\n",
    "        elif i != 0:\n",
    "            print(f'Possible circular route: {loc} -> {stop}, at index {i}')\n",
    "        i += 1\n",
    "    # Verify solution is legit\n",
    "    if i == NumElements:\n",
    "        print(\n",
    "            f'Route contains {i} stops out of {NumElements} locations, and is indeed circular!')\n",
    "    else:\n",
    "        print(\n",
    "            f'Route is not circular, as there are {i} stops and {NumElements} locations')\n",
    "    print(routes)\n",
    "    return routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is good, but a graph is better\n",
    "It is undeniable that a graph is much more succint and easier to read than a list of print statements, at least when looking at a route. Therefore, use Plotly Mapbox for ease of use/interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def APIMANAGER(js: str) -> pd.DataFrame:\n",
    "    ''' Gets all of the locations into a pretty format\n",
    "\n",
    "    # Arguments #\n",
    "    :arg js: return from the TomTom url that needs interpreting\n",
    "    :type js: str\n",
    "    \n",
    "    # Returns #\n",
    "    :ret loc_df: Full route coordinates for the route inside js\n",
    "    :rtype loc_df: pd.DataFrame\n",
    "    '''\n",
    "    jsDump = json.dumps(js)\n",
    "    jsLoad = json.loads(jsDump)\n",
    "    points = jsLoad['routes'][0]['legs'][0]['points']\n",
    "    coords = []\n",
    "    for item in points:\n",
    "        coords.append([float(item['longitude']), float(item['latitude'])])\n",
    "    loc_df = pd.DataFrame(coords, columns=['Longitude', 'Latitude'])\n",
    "    return loc_df\n",
    "\n",
    "def routeGenerator(startLat: float, startLon: float, endLat: float, endLon: float, apiKey: str) -> pd.DataFrame:\n",
    "    ''' Takes a start and end point, and produces a full route\n",
    "\n",
    "    # Arguments #\n",
    "    :arg startLat: Latitude for starting location\n",
    "    :type startLat: float\n",
    "    :arg startLon: Longitude for starting location\n",
    "    :type startLon: float\n",
    "    :arg endLat: Latitude for ending location\n",
    "    :type endLat: float    \n",
    "    :arg endLon: Longitude for ending location\n",
    "    :type endLon: float\n",
    "    :arg apiKey: TomTom api key\n",
    "    :type apiKey: str\n",
    "\n",
    "    # Returns #\n",
    "    :ret df: All of the coordinates needed for the route\n",
    "    :rtype df: pd.DataFrame\n",
    "    '''\n",
    "    tomtomURL = f'https://api.tomtom.com/routing/1/calculateRoute/{startLat},{startLon}:{endLat},{endLon}/json?maxAlternatives=0&routeType=shortest&travelMode=bicycle&key={apiKey}'\n",
    "    getData = requests.get(tomtomURL)\n",
    "    while (getData.status_code != 200):\n",
    "        getData = requests.get(tomtomURL)\n",
    "    jsonTomTomString = getData.json()\n",
    "    start = pd.DataFrame({'Latitude':startLat,'Longitude':startLon}, index =[0])\n",
    "    df = APIMANAGER(jsonTomTomString)\n",
    "    end = pd.DataFrame({'Latitude':endLat,'Longitude':endLon}, index =[0])\n",
    "    df = pd.concat([start,df[:],end]).reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def GenerateMapSolutions(sol: dict, dataframe: pd.DataFrame, apiKey: str) -> dict:\n",
    "    \"\"\"Creates a pandas dataframe that represents a solution from the response given by a .sol file\n",
    "\n",
    "    # Arguments #\n",
    "    :arg sol: A full circular route\n",
    "    :type response: dict\n",
    "    :arg dataframe: A dataframe consisting of at least location Longitude and Latitudes.\n",
    "    :type dataframe: pd.DataFrame\n",
    "    :arg apiKey: apiKey for TomTom\n",
    "    :type apiKey: str\n",
    "\n",
    "    # Returns #\n",
    "    :return: A dataframe which represents a series of points of different coordinates and colors that form a route.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    retVal = {}\n",
    "    for i in sol.keys():\n",
    "        startLat = dataframe.iloc[i][\"Latitude\"].item()\n",
    "        startLon = dataframe.iloc[i][\"Longitude\"].item()\n",
    "        endLat = dataframe.iloc[sol[i]][\"Latitude\"].item()\n",
    "        endLon = dataframe.iloc[sol[i]][\"Longitude\"].item()\n",
    "        print((startLat, startLon), \"->\", (endLat, endLon))\n",
    "        retVal[i] = routeGenerator(startLat, startLon, endLat, endLon, apiKey)\n",
    "    return retVal\n",
    "\n",
    "\n",
    "def make_map(pathingList:dict, locations:pd.DataFrame, mapboxKey:str, sol:dict) -> None:\n",
    "    ''' Now that all the prep is done, actually make the darn map\n",
    "\n",
    "    # Arguments #\n",
    "    :arg pathingList: Nested dataframes with all of the route coordinates\n",
    "    :type pathingList: dict\n",
    "    :arg locations: all of the locations that will be visited\n",
    "    :type locations: pd.DataFrame\n",
    "    :arg mapboxKey: key to access Plotly Mapbox\n",
    "    :type mapboxKey: str\n",
    "    :arg sol: Ordered solution, so that the routes can be labeled\n",
    "    :type sol: dict\n",
    "    '''\n",
    "    fig = go.Figure(go.Scattergeo())\n",
    "    # Plot all locations\n",
    "    lat = locations['Latitude'].values.tolist()\n",
    "    lon = locations['Longitude'].values.tolist()\n",
    "    fig.add_trace(go.Scattermapbox(\n",
    "        lat=lat,\n",
    "        lon=lon,\n",
    "        name='Locations'\n",
    "    ))\n",
    "    # Plot all routes\n",
    "    for i in pathingList.keys():\n",
    "        start = locations.iloc[i]['Name']\n",
    "        end = locations.iloc[sol[i]]['Name']\n",
    "        fig.add_trace(go.Scattermapbox(\n",
    "            mode='lines',\n",
    "            lat=pathingList[i]['Latitude'].values.tolist(),\n",
    "            lon=pathingList[i][\"Longitude\"].values.tolist(),\n",
    "            name = f'{start} -> {end}'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Using Mapbox\n",
    "    fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "    fig.update_layout(mapbox_style=\"light\", mapbox_accesstoken=mapboxKey)\n",
    "    # Display map\n",
    "    plt.offline.plot(\n",
    "        fig,\n",
    "        filename=os.path.join(os.getcwd(), \"Maps\", f\"GeneratedMap.html\"),\n",
    "        auto_open=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def ShowMapSolutions(sol: dict, dataframe: pd.DataFrame, TomTomKey: str, MapBoxKey: str):\n",
    "    \"\"\"Generates map soutions from a .sol file\n",
    "\n",
    "    :param sol: A route dictionary\n",
    "    :type Solutions: dict\n",
    "    :param dataframe: A dataframe consisting of at least location Longitude and Latitudes.\n",
    "    :type dataframe: pd.DataFrame\n",
    "    :param TomTomKey: API key for TomTom, the route provider\n",
    "    :type TomTomKey: str\n",
    "    :param MapBoxKey: API key for Plotly Mapbox, the route grapher\n",
    "    :type MapBoxKey: str\n",
    "    \"\"\"\n",
    "    if os.path.exists(MAP_PATH):\n",
    "        shutil.rmtree(MAP_PATH)\n",
    "    os.mkdir(MAP_PATH)\n",
    "    GeneratedSolution = GenerateMapSolutions(sol, dataframe, TomTomKey)\n",
    "    make_map(GeneratedSolution, dataframe, MapBoxKey, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example graphs, LP files, and data, please reference [this google drive folder](https://drive.google.com/drive/folders/1wWpx6V2ewvnvsE55xQsGaKLdma_f0OjR?usp=sharing)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ce311k')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f0a2c10ae07ef1a20c42b873bb358e5a4a9f8dc63953484f3676dbc84a47338"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
