{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "FILENAME = \"AddressesTest\"\n",
    "\n",
    "# Open API Key\n",
    "with open(\"Keys.json\", \"r\") as data:\n",
    "    Keys = json.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Known Errors\n",
    "1. Subtour elimination constraint: Work in progress, currently returns an infeasible solve.\n",
    "            - However, the model will solve without it, but does return a route with subtours\n",
    "\n",
    "### Next Up In Development\n",
    "1. Finish subtour elimination constraint\n",
    "    - Matter of finishing figuring out how to write it, variable generation should be done\n",
    "2. .sol file interpretation\n",
    "    - This is assuming that the LP files get manually uploaded to NEOS for solving\n",
    "    - CPLEX there can return a .sol file that we can parse and interpret solutions from\n",
    "3. Data display\n",
    "    - A project is no good if you can't understand the results\n",
    "    - Still need to find a good way of showing the solution on a graph\n",
    "    - Could be a quick and dirty NetworkX thing, or a more sophisticated MapBox solution\n",
    "4. If time, remote solve\n",
    "    - Currently make LP file, then upload it manually to solve\n",
    "    - Would be nice to instead change it into an xml approach that can be sent to NEOS via script\n",
    "    - Removes time and human error, also allows for everything to be done in one script, not two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define API Call Function\n",
    "def RouteCaller(loc1:str, loc2:str,  key:str=None, units:str='meters', routeType:str='BICYCLE') -> float:\n",
    "    \"\"\" Function that makes a single API call between two distances\n",
    "\n",
    "    # Arguments #\n",
    "    :arg loc1: Starting location for the route\n",
    "    :type loc1: str\n",
    "    :arg loc2: Ending location for the route\n",
    "    :type loc2: str\n",
    "    :arg units: Measuring system for the route, metric of imperial\n",
    "    :type units: str\n",
    "    :arg routeType: Mode of travel for the route. Default is Bicyle because that's the purpose of this project\n",
    "    :type routeType: str\n",
    "    \"\"\"\n",
    "    print(f'Getting data for {loc1} to {loc2}')\n",
    "    # Verify Key exists\n",
    "    if key == None:\n",
    "        raise ValueError('No API Key has been passed. Please insert key and try again')\n",
    "    # Ping the API\n",
    "    retVal = requests.get(\n",
    "        f\"https://maps.googleapis.com/maps/api/directions/json?destination={loc1}&origin={loc2}&units={units}&mode={routeType}&key={key}\").json()\n",
    "    if 'error_message' in retVal.keys():\n",
    "        # Error handling\n",
    "        print(\n",
    "            f\"Error when pinging API, issue: {retVal['error_message']}\")\n",
    "        return np.NaN\n",
    "    else:\n",
    "        # Return the route information\n",
    "        return float(retVal['routes'][0]['legs'][0]['distance']['text'][:-3])\n",
    "\n",
    "# Define the Distance Matrix\n",
    "def generateDistanceMatrix(df:pd.DataFrame, key:str) -> np.ndarray:\n",
    "    \"\"\" Takes in a DataFrame, and returns a numpy array\n",
    "\n",
    "    # Arguments #\n",
    "    :arg df: all of the addresses in the problem, placed in a pandas DataFrame\n",
    "    :type df: pd.DataFrame\n",
    "    :arg key: API key, so that Google knows who is calling\n",
    "    :type key: str\n",
    "\n",
    "    # Returns #\n",
    "    :return distMatrix: A matrix of size NxN, where N is the number of locations in DataFrame df\n",
    "    :rtype distMatrix: np.ndarray\n",
    "    \"\"\"\n",
    "    # Local array storage\n",
    "    distDict = {}\n",
    "    arrays = []\n",
    "\n",
    "    # Iterate through the DataFrame\n",
    "    for index1, row1 in df.iterrows():\n",
    "        distDict[index1] = []\n",
    "        for index2, row2 in df.iterrows():\n",
    "            if index1 == index2:\n",
    "                # If location equals itself then give 0 for distance, no need to ping the API\n",
    "                distDict[index1].append(0)\n",
    "\n",
    "            elif index1 != index2:\n",
    "                # Ping Google's Destination API for route data\n",
    "                distDict[index1].append(RouteCaller(df.iloc[index1]['Address'], df.iloc[index2]['Address'], key))\n",
    "\n",
    "    # Process data into arrays, stack, and return\n",
    "    for key in distDict.keys():\n",
    "        arrays.append(np.array(distDict[key]))\n",
    "    distMatrix = np.vstack(arrays)\n",
    "    return distMatrix\n",
    "\n",
    "\n",
    "def validateCache(filename:str,df:pd.DataFrame,key:str=None) -> np.ndarray:\n",
    "    \"\"\" Validate and load cached data, to prevent unnecessary API calls\n",
    "\n",
    "    # Arguments #\n",
    "    :arg filename: ending of filename to check if it exits. If it doesn't we will end up making it\n",
    "    :type filename: str\n",
    "    :arg df: If no cache, need data to pass to route generator\n",
    "    :type df: pd.DataFrame\n",
    "    :arg key: If no cache, need key for API call\n",
    "    :type key: str\n",
    "\n",
    "    # Returns #\n",
    "    :return distMatrix: Returns a distmatrix\n",
    "    :rtype distMatrix: np.ndarray\n",
    "    \"\"\"\n",
    "    # Define where the cache should be\n",
    "    cache_filename = os.path.splitext(filename)[0] + \".npy\"\n",
    "    # Check for if the parent folder exists, if not make one\n",
    "    if not os.path.exists(os.path.join(os.path.dirname(__file__),\"CachedDistances\")):\n",
    "        os.mkdir(os.path.join(os.path.dirname(__file__),\"CachedDistances\"))\n",
    "    # Check if the distance matrix doesn't exist, if so make one\n",
    "    if not os.path.exists(os.path.join(os.path.dirname(__file__),\"CachedDistances\",cache_filename)):\n",
    "        distMatrix = generateDistanceMatrix(df, key)\n",
    "        np.save(os.path.join(os.path.dirname(__file__),\"CachedDistances\",cache_filename),distMatrix)\n",
    "        return distMatrix\n",
    "    # Since the matrix exists, just load it\n",
    "    else:\n",
    "        distMatrix = np.load(os.path.join(os.path.dirname(__file__), \"CachedDistances\", cache_filename))\n",
    "        return distMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateContraintMatrix(distMatrix:np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Generate the constraint matrix from given distance matrix\n",
    "\n",
    "    # Arguments #\n",
    "    :arg distMatrix: Big distance matrix that we calculated in the last step\n",
    "    :type distMatrix: np.ndarray\n",
    "\n",
    "    # Returns #\n",
    "    :return retVal: Combined constraint matrix, with top and bottom halves stacked nicely\n",
    "    :rtype retVal: np.ndarray\n",
    "    \"\"\"\n",
    "    NumElements = len(distMatrix)\n",
    "    \"\"\"\n",
    "        Generates the top portion of the constraint matrix, a 1 x N matrix of ones for index i in N. Here is an example with N = 3.\n",
    "        [ 0 0 0 0 0 0 0 0 0 ]        [ 1 1 1 0 0 0 0 0 0 ]\n",
    "        | 0 0 0 0 0 0 0 0 0 | --->   | 0 0 0 1 1 1 0 0 0 |\n",
    "        [ 0 0 0 0 0 0 0 0 0 ]        [ 0 0 0 0 0 0 1 1 1 ]\n",
    "    \"\"\"\n",
    "    retArrayTop = np.zeros(shape=(NumElements,NumElements**2))\n",
    "    for index in range(NumElements):\n",
    "        retArrayTop[index,index*NumElements:(index+1)*NumElements] = np.ones(shape=(1,NumElements))\n",
    "    \"\"\"\n",
    "        Generates the bottom portion of the constraint matrix, a N x N**2 matrix full of horizontally aligned identity N x N matricies.\n",
    "        Example with N = 3.\n",
    "        [ 1 0 0 ]   [ 1 0 0 ]   [ 1 0 0 ]         [ 1 0 0 1 0 0 1 0 0 ]\n",
    "        | 0 1 0 | + | 0 1 0 | + | 0 1 0 |   --->  | 0 1 0 0 1 0 0 1 0 |\n",
    "        [ 0 0 1 ]   [ 0 0 1 ]   [ 0 0 1 ]         [ 0 0 1 0 0 1 0 0 1 ]\n",
    "    \"\"\"\n",
    "    individualBottoms = tuple(np.identity(NumElements) for i in range(NumElements))\n",
    "    retArrayBottom = np.hstack(individualBottoms)\n",
    "    retVal = np.vstack((retArrayTop,retArrayBottom))\n",
    "    #retVal = np.vstack((retVal,np.ones(shape=(1,NumElements**2),dtype=np.uint8)))\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to LP File\n",
    "This is the most jank part, and needs optimization and clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpGenerator(distMatrix:np.ndarray, constraintMatrix:np.ndarray,filename:str) -> np.ndarray:\n",
    "    \"\"\" Writes the lp file for the constraint matrix. LP Files are the way we give the solver our problem.\n",
    "\n",
    "    # Arguments #\n",
    "    :arg distMatrix: a distance matrix of size N x N. This contains the necessary objective information\n",
    "    :type distMatrix: np.ndarray\n",
    "    :arg constraintMatrix: the constraint matrix with binary variables\n",
    "    :type constraintMatrix: np.ndarray\n",
    "    :arg filename: input filename that we will make the associated lp file for\n",
    "    :type filename: str\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define where the LP file should be\n",
    "    lp_filename = os.path.splitext(filename)[0] + \".lp\"\n",
    "    # Check for if the parent folder exists, if not make one\n",
    "    if not os.path.exists(os.path.join(os.path.dirname(__file__),\"LPFiles\")):\n",
    "        os.mkdir(os.path.join(os.path.dirname(__file__),\"LPFiles\"))\n",
    "    full_lp_filename = os.path.join(os.path.dirname(__file__),\"LPFiles\",lp_filename)\n",
    "    # Delete old lp file of same name\n",
    "    if os.path.exists(full_lp_filename):\n",
    "        os.remove(full_lp_filename)\n",
    "\n",
    "    # Find N\n",
    "    NumElements = len(distMatrix)\n",
    "\n",
    "    # Variable Creation\n",
    "    vars_dict = {}\n",
    "    for i in range(NumElements):\n",
    "        vars_dict[i] = {}\n",
    "        for j in range(NumElements):\n",
    "            vars_dict[i][j] = f'X{i}Y{j}'\n",
    "    subtour = [f'U{i}' for i in range(NumElements)]\n",
    "    \"\"\"\n",
    "    An LP File stands for Linear Programming File. This is the input that solvers like CPLEX prefer to take in.\n",
    "    LP Files are organized into logical sections. Each section is started by a keyword.\n",
    "    The first section is the objective, and has keywords: max, maximum, maximize, min, minimum, minimize.\n",
    "    \"\"\"\n",
    "    # Write out cost * variable, put in a list\n",
    "    costs = []\n",
    "    for i in vars_dict.keys():\n",
    "        for j in vars_dict[i].keys():\n",
    "            cost = distMatrix[i, j]\n",
    "            if cost != 0.0:\n",
    "                costs.append(f'{cost} {vars_dict[i][j]}')\n",
    "                costs.append('+')\n",
    "    costs.pop()\n",
    "    print(costs)\n",
    "    '''\n",
    "    The second section is the constraint section. This is where we tell the solver the rules that it must\n",
    "    follow in order to obtain a feasible solution. We start this section using \"subject to\"\n",
    "    '''\n",
    "    lp_constraints = []\n",
    "    # Top half\n",
    "    loc = 0\n",
    "    for i in vars_dict.keys():\n",
    "        local = []\n",
    "        for j in vars_dict[i].keys():\n",
    "            if i != j:\n",
    "                if constraintMatrix[i,j+loc] == 1.0:\n",
    "                    local.append(vars_dict[i][j])\n",
    "                    local.append(' + ')\n",
    "        local.pop()\n",
    "        lp_constraints.append(local)\n",
    "        loc += NumElements\n",
    "    # Bottom Half\n",
    "    loc = 0\n",
    "    for i in vars_dict.keys():\n",
    "        local = []\n",
    "        for j in vars_dict[i].keys():\n",
    "            if i != j:\n",
    "                if constraintMatrix[i+NumElements,j*NumElements+loc] == 1.0:\n",
    "                    local.append(vars_dict[j][i])\n",
    "                    local.append(' + ')\n",
    "        local.pop()\n",
    "        lp_constraints.append(local)\n",
    "        loc += 1\n",
    "\n",
    "    # Write the LP file\n",
    "    with open(full_lp_filename, 'x') as lp:\n",
    "        # Objective Section\n",
    "        lp.write('Min \\n')\n",
    "        for eq in costs:\n",
    "            lp.write(f'{eq} ')\n",
    "        # Constraint Section\n",
    "        lp.write('; \\nsubject to \\n')\n",
    "        for eq in lp_constraints:\n",
    "            for i in eq:\n",
    "                lp.write(i)\n",
    "            lp.write(' = 1 \\n')\n",
    "        # Subtour elimination\n",
    "        for i in vars_dict.keys():\n",
    "            for j in vars_dict.keys():\n",
    "                lp.write(f'{subtour[j]} - {subtour[i]} - {vars_dict[i][j]} >= {2*NumElements-1} \\n')\n",
    "        # Define Variable Types\n",
    "        lp.write('bin \\n')\n",
    "        for i in vars_dict.keys():\n",
    "            for j in vars_dict[i].keys():\n",
    "                if i != j:\n",
    "                    lp.write(f'{vars_dict[i][j]} ')\n",
    "        for i in subtour:\n",
    "            lp.write(f'{i} ')\n",
    "        lp.write('\\nEND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define main, run file\n",
    "This has all been done in a main.py file so far, and has not been directly adapted for a jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filename:str,key:str=None):\n",
    "    \"\"\" We put all the work inside this one function, so that at the end of this program we only have to call one function\n",
    "\n",
    "    # Arguments #\n",
    "    :arg filename: Generic name of the input data file\n",
    "    :type filename: str\n",
    "    :arg key: API key needed to call Google Maps if no cached data\n",
    "    :type key: str\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the data, put it in Pandas\n",
    "    with open(f\"Data/{FILENAME}.csv\", \"r\") as data:\n",
    "        df = pd.read_csv(data)\n",
    "        # Make the addresses API friendly\n",
    "        df = df.replace(' ', '+', regex=True)\n",
    "    # Get distance matrix\n",
    "    distMatrix = validateCache(filename,df,key)\n",
    "    constMatrix = generateContraintMatrix(distMatrix)\n",
    "    lpGenerator(distMatrix, constMatrix, filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(filename=FILENAME, key=Keys['googleMaps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually call main I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(filename=FILENAME, key=Keys['googleMaps'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ce311k')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f0a2c10ae07ef1a20c42b873bb358e5a4a9f8dc63953484f3676dbc84a47338"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
